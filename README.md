# tabby-community-images

A community-maintained CPU-only Docker image for [TabbyML Tabby](https://github.com/TabbyML/tabby).

## About

This project provides a Docker image for running Tabby with CPU-only support. The official Tabby project currently focuses on GPU-accelerated images, so this repository fills the gap for users who want to run Tabby on CPU-only infrastructure.

**Note on ROCm:** While ROCm images are built, they have not been tested. Use at your own risk.

> **Note:** This is a community-maintained project and is not officially supported by TabbyML. For the official Tabby project, visit [TabbyML/tabby](https://github.com/TabbyML/tabby).

## Quick Start

### Basic Usage

```bash
docker run -it \
  --name tabby \
  -p 8080:8080 \
  -v $HOME/.tabby:/data \
  ghcr.io/nilgaar/tabby-community-images-cpu:latest \
  serve --model StarCoder-1B --device cpu
```

Then visit `http://localhost:8080` to access the Tabby web interface.

### Download a Model

Before running Tabby, you need to download a model. Here are some recommended models for CPU:

```bash
# Small and fast (recommended for most CPU setups)
docker run --rm -v $HOME/.tabby:/data \
  ghcr.io/nilgaar/tabby-community-images-cpu:latest \
  download --model StarCoder-1B

# Better quality, requires more resources
docker run --rm -v $HOME/.tabby:/data \
  ghcr.io/nilgaar/tabby-community-images-cpu:latest \
  download --model Qwen2.5-Coder-1.5B

# High quality, for powerful CPUs
docker run --rm -v $HOME/.tabby:/data \
  ghcr.io/nilgaar/tabby-community-images-cpu:latest \
  download --model Qwen2.5-Coder-3B
```

## Docker Compose Setup

### Standalone Deployment

Create a `docker-compose.yml` file:

```yaml
services:
  tabby:
    image: ghcr.io/nilgaar/tabby-community-images-cpu:latest
    container_name: tabby
    restart: unless-stopped
    command: serve --model Qwen2.5-Coder-1.5B --device cpu
    ports:
      - "8080:8080"
    volumes:
      - tabby-data:/data
    environment:
      - TABBY_ROOT=/data
      - TABBY_WEBSERVER_JWT_TOKEN_SECRET=your-secure-uuid-here # Generate with: uuidgen

volumes:
  tabby-data:
```

Then run:

```bash
docker compose up -d
```

### Production Deployment with Traefik

For a production setup with automatic HTTPS via Traefik reverse proxy:

```yaml
services:
  tabby:
    image: ghcr.io/nilgaar/tabby-community-images-cpu:latest
    container_name: tabby-prod
    restart: unless-stopped
    command: serve --model Qwen2.5-Coder-1.5B --device cpu
    volumes:
      - tabby-data:/data
    environment:
      - TABBY_ROOT=/data
      - RUST_BACKTRACE=1
      - RUST_LOG=info
      - TABBY_WEBSERVER_JWT_TOKEN_SECRET=your-secure-uuid-here # Generate with: uuidgen
    networks:
      - traefik
    deploy:
      resources:
        limits:
          cpus: "12"
          memory: 24G
        reservations:
          cpus: "4"
          memory: 8G
    shm_size: 4gb
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.tabby.rule=Host(`tabby.yourdomain.com`)"
      - "traefik.http.routers.tabby.entrypoints=websecure"
      - "traefik.http.routers.tabby.tls.certresolver=letsencrypt"
      - "traefik.http.services.tabby.loadbalancer.server.port=8080"
      - "com.centurylinklabs.watchtower.enable=true"

  watchtower:
    image: containrrr/watchtower
    container_name: tabby-watchtower
    restart: always
    networks:
      - traefik
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_POLL_INTERVAL=300
      - WATCHTOWER_INCLUDE_STOPPED=true
      - WATCHTOWER_INCLUDE_RESTARTING=true
      - WATCHTOWER_ROLLING_RESTART=true
      - WATCHTOWER_LIFECYCLE_HOOKS=true
      - WATCHTOWER_TRACE_LEVEL=info
    command: --revive-stopped --label-enable

networks:
  traefik:
    external: true

volumes:
  tabby-data:
```

**Before deploying:**

1. Replace `your-secure-uuid-here` with a UUID generated by `uuidgen`
2. Replace `tabby.yourdomain.com` with your actual domain
3. Ensure you have Traefik configured with Let's Encrypt certificate resolver named `letsencrypt`
4. Create the external traefik network: `docker network create traefik`

## Configuration

### Available Models

See the [Tabby model registry](https://tabby.tabbyml.com/docs/models/) for the complete list.

### Environment Variables

- `TABBY_ROOT` - Data directory (default: `/data`)
- `TABBY_WEBSERVER_JWT_TOKEN_SECRET` - JWT secret for session management (use UUID format)
- `RUST_BACKTRACE` - Enable Rust backtraces for debugging (1 or full)
- `RUST_LOG` - Logging level (info, debug, warn, error)

## IDE Integration

### VSCode

1. Install the [Tabby extension](https://marketplace.visualstudio.com/items?itemName=TabbyML.vscode-tabby)
2. Configure the server endpoint:
   - Open VSCode settings
   - Search for "Tabby"
   - Set "Tabby: Endpoint" to your Tabby server URL (e.g., `https://tabby.yourdomain.com`)
3. Reload VSCode

### JetBrains IDEs

1. Install the [Tabby plugin](https://plugins.jetbrains.com/plugin/22379-tabby)
2. Configure in Settings → Tools → Tabby
3. Set server URL and restart IDE

## Troubleshooting

### Container keeps restarting

Check logs: `docker logs tabby`

Common issues:

- Model not specified in command
- Insufficient memory
- Invalid JWT secret format (must be UUID)

### Autocomplete not working

1. Verify model is loaded: `docker logs tabby | grep -i model`
2. Check IDE extension is connected to correct endpoint
3. Ensure you've created an account in the web UI
4. Verify the model downloaded successfully

### Health check failing

The default health check may fail if the `/v1/health` endpoint requires authentication. Use this custom health check:

```yaml
healthcheck:
  test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
```

## Building from Source

See the [original repository](https://github.com/nilgaar/tabby-community-images) for build instructions.

## License

This project follows the same Apache 2.0 license as [TabbyML Tabby](https://github.com/TabbyML/tabby).

## Contributing

Contributions are welcome! Please open an issue or pull request on the [GitHub repository](https://github.com/nilgaar/tabby-community-images).

## Acknowledgments

- [TabbyML](https://github.com/TabbyML/tabby) for creating Tabby
- The open-source community for model development
